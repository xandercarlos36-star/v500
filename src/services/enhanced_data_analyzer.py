#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Enhanced Data Analyzer
An√°lise aprimorada e profunda dos dados extra√≠dos
"""

import os
import logging
import time
import json
import re
from typing import Dict, List, Any, Optional
from datetime import datetime
from collections import Counter
from services.ai_manager import ai_manager
from services.auto_save_manager import salvar_etapa, salvar_erro

logger = logging.getLogger(__name__)

class EnhancedDataAnalyzer:
    """Analisador aprimorado de dados extra√≠dos com an√°lise profunda"""

    def __init__(self):
        """Inicializa o analisador aprimorado"""
        self.analysis_depth_levels = {
            'surface': 1,
            'intermediate': 2,
            'deep': 3,
            'ultra_deep': 4
        }

        self.analysis_components = [
            'content_analysis',
            'sentiment_analysis',
            'trend_analysis',
            'competitor_analysis',
            'opportunity_analysis',
            'audience_analysis',
            'market_analysis',
            'behavioral_analysis'
        ]

        logger.info("üî¨ Enhanced Data Analyzer inicializado")

    def perform_enhanced_analysis(
        self,
        extracted_data: Dict[str, Any],
        session_id: str,
        analysis_depth: str = "ultra_deep",
        processing_time_limit: Optional[int] = None
    ) -> Dict[str, Any]:
        """Executa an√°lise aprimorada dos dados extra√≠dos"""

        try:
            logger.info(f"üöÄ Iniciando an√°lise APRIMORADA com n√≠vel: {analysis_depth}")
            start_time = time.time()

            analysis_results = {
                'session_id': session_id,
                'analysis_depth': analysis_depth,
                'timestamp': datetime.now().isoformat(),
                'input_data_summary': self._summarize_input_data(extracted_data),
                'detailed_analysis': {},
                'consolidated_insights': {},
                'processing_metrics': {}
            }

            # FASE 1: An√°lise de conte√∫do aprimorada
            logger.info("üìù Executando an√°lise de conte√∫do aprimorada...")
            content_analysis = self._perform_content_analysis(extracted_data, session_id)
            analysis_results['detailed_analysis']['content_analysis'] = content_analysis

            # FASE 2: An√°lise de sentimento profunda
            logger.info("üòä Executando an√°lise de sentimento profunda...")
            sentiment_analysis = self._perform_sentiment_analysis(extracted_data, session_id)
            analysis_results['detailed_analysis']['sentiment_analysis'] = sentiment_analysis

            # FASE 3: An√°lise de tend√™ncias
            logger.info("üìà Executando an√°lise de tend√™ncias...")
            trend_analysis = self._perform_trend_analysis(extracted_data, session_id)
            analysis_results['detailed_analysis']['trend_analysis'] = trend_analysis

            # FASE 4: An√°lise competitiva
            logger.info("ü•ä Executando an√°lise competitiva...")
            competitor_analysis = self._perform_competitor_analysis(extracted_data, session_id)
            analysis_results['detailed_analysis']['competitor_analysis'] = competitor_analysis

            # FASE 5: An√°lise de oportunidades
            logger.info("üí° Executando an√°lise de oportunidades...")
            opportunity_analysis = self._perform_opportunity_analysis(extracted_data, session_id)
            analysis_results['detailed_analysis']['opportunity_analysis'] = opportunity_analysis

            # FASE 6: An√°lise de audi√™ncia
            logger.info("üë• Executando an√°lise de audi√™ncia...")
            audience_analysis = self._perform_audience_analysis(extracted_data, session_id)
            analysis_results['detailed_analysis']['audience_analysis'] = audience_analysis

            # FASE 7: An√°lise de mercado
            logger.info("üìä Executando an√°lise de mercado...")
            market_analysis = self._perform_market_analysis(extracted_data, session_id)
            analysis_results['detailed_analysis']['market_analysis'] = market_analysis

            # FASE 8: An√°lise comportamental
            logger.info("üß† Executando an√°lise comportamental...")
            behavioral_analysis = self._perform_behavioral_analysis(extracted_data, session_id)
            analysis_results['detailed_analysis']['behavioral_analysis'] = behavioral_analysis

            # FASE 9: Consolida√ß√£o de insights
            logger.info("üîó Consolidando insights...")
            consolidated_insights = self._consolidate_insights(analysis_results['detailed_analysis'], session_id)
            analysis_results['consolidated_insights'] = consolidated_insights

            # M√©tricas de processamento
            processing_time = time.time() - start_time
            analysis_results['processing_metrics'] = {
                'processing_time_seconds': processing_time,
                'analysis_components_executed': len(analysis_results['detailed_analysis']),
                'total_insights_generated': len(consolidated_insights.get('key_insights', [])),
                'data_quality_score': self._calculate_data_quality_score(analysis_results)
            }

            # Salva an√°lise completa
            salvar_etapa(
                "enhanced_data_analysis_complete",
                analysis_results,
                session_id=session_id,
                categoria="data_analysis"
            )

            logger.info(f"‚úÖ An√°lise aprimorada conclu√≠da em {processing_time:.2f}s")
            return analysis_results

        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise aprimorada: {e}")
            salvar_erro("enhanced_analysis_error", e, contexto={
                'session_id': session_id,
                'analysis_depth': analysis_depth
            })

            return {
                'session_id': session_id,
                'error': str(e),
                'success': False,
                'timestamp': datetime.now().isoformat()
            }

    def _summarize_input_data(self, extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """Gera resumo dos dados de entrada"""

        summary = {
            'total_data_sources': 0,
            'platforms_analyzed': [],
            'content_types_found': [],
            'data_volume_score': 0
        }

        try:
            # Analisa dados consolidados
            if 'consolidated_data' in extracted_data:
                consolidated = extracted_data['consolidated_data']
                summary['total_data_sources'] = len(consolidated)
                summary['platforms_analyzed'] = list(consolidated.keys())

                total_items = sum(len(data) for data in consolidated.values() if isinstance(data, list))
                summary['data_volume_score'] = min(100, total_items * 2)

            # Identifica tipos de conte√∫do
            content_types = set()
            for platform_data in extracted_data.get('consolidated_data', {}).values():
                if isinstance(platform_data, list):
                    for item in platform_data:
                        if 'platform' in item:
                            content_types.add(item['platform'])
                        if 'url' in item:
                            if 'youtube.com' in item['url']:
                                content_types.add('video')
                            elif 'instagram.com' in item['url']:
                                content_types.add('social_post')
                            elif 'tiktok.com' in item['url']:
                                content_types.add('short_video')

            summary['content_types_found'] = list(content_types)

        except Exception as e:
            logger.error(f"Erro ao resumir dados de entrada: {e}")
            summary['error'] = str(e)

        return summary

    def _perform_content_analysis(self, extracted_data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Realiza an√°lise aprimorada de conte√∫do"""

        try:
            # Extrai todo o texto dispon√≠vel
            all_text_content = []
            all_titles = []

            for platform_data in extracted_data.get('consolidated_data', {}).values():
                if isinstance(platform_data, list):
                    for item in platform_data:
                        if 'text' in item and item['text']:
                            all_text_content.append(item['text'])
                        if 'title' in item and item['title']:
                            all_titles.append(item['title'])
                        if 'description' in item and item['description']:
                            all_text_content.append(item['description'])

            # An√°lise de frequ√™ncia de palavras
            word_frequency = self._analyze_word_frequency(' '.join(all_text_content + all_titles))

            # An√°lise de temas usando IA
            themes_prompt = f"""
            Analise o seguinte conte√∫do extra√≠do de redes sociais e identifique os principais temas e padr√µes:

            T√çTULOS ENCONTRADOS:
            {chr(10).join(all_titles[:50])}

            CONTE√öDO EXTRA√çDO:
            {chr(10).join(all_text_content[:20])}

            Identifique:
            1. Principais temas discutidos
            2. Padr√µes de linguagem
            3. Tipos de conte√∫do mais frequentes
            4. Tend√™ncias de engajamento
            5. Oportunidades de conte√∫do identificadas

            Responda em formato JSON com essas categorias.
            """

            themes_analysis = ai_manager.generate_content(
                themes_prompt,
                max_tokens=2000,
                temperature=0.3
            )

            content_analysis = {
                'total_content_pieces': len(all_text_content) + len(all_titles),
                'word_frequency_top_20': word_frequency[:20],
                'themes_analysis': themes_analysis,
                'content_volume_metrics': {
                    'total_characters': sum(len(text) for text in all_text_content),
                    'average_content_length': sum(len(text) for text in all_text_content) / max(len(all_text_content), 1),
                    'title_count': len(all_titles),
                    'text_content_count': len(all_text_content)
                }
            }

            return content_analysis

        except Exception as e:
            logger.error(f"Erro na an√°lise de conte√∫do: {e}")
            return {'error': str(e)}

    def _perform_sentiment_analysis(self, extracted_data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Realiza an√°lise de sentimento profunda"""

        try:
            sentiment_data = []

            for platform, platform_data in extracted_data.get('consolidated_data', {}).items():
                if isinstance(platform_data, list):
                    for item in platform_data:
                        text_to_analyze = []

                        if 'title' in item and item['title']:
                            text_to_analyze.append(item['title'])
                        if 'text' in item and item['text']:
                            text_to_analyze.append(item['text'])
                        if 'description' in item and item['description']:
                            text_to_analyze.append(item['description'])

                        if text_to_analyze:
                            combined_text = ' '.join(text_to_analyze)

                            # An√°lise de sentimento usando IA
                            sentiment_prompt = f"""
                            Analise o sentimento do seguinte texto de rede social:
                            "{combined_text[:300]}"

                            Classifique como:
                            - Positivo (score: 0.6-1.0)
                            - Neutro (score: 0.2-0.6)
                            - Negativo (score: 0.0-0.2)

                            Responda apenas com um n√∫mero entre 0.0 e 1.0 representando o score de positividade.
                            """

                            try:
                                sentiment_result = ai_manager.generate_content(
                                    sentiment_prompt,
                                    max_tokens=50,
                                    temperature=0.1
                                )

                                # Extrai score num√©rico
                                score = self._extract_sentiment_score(sentiment_result)

                                sentiment_data.append({
                                    'platform': platform,
                                    'text_preview': combined_text[:100],
                                    'sentiment_score': score,
                                    'sentiment_label': self._score_to_label(score)
                                })

                            except Exception as e:
                                logger.warning(f"Erro na an√°lise de sentimento individual: {e}")

            # Calcula estat√≠sticas gerais
            if sentiment_data:
                scores = [item['sentiment_score'] for item in sentiment_data if 'sentiment_score' in item]

                sentiment_analysis = {
                    'total_analyzed': len(sentiment_data),
                    'average_sentiment': sum(scores) / len(scores) if scores else 0.5,
                    'sentiment_distribution': {
                        'positive': len([s for s in scores if s > 0.6]),
                        'neutral': len([s for s in scores if 0.2 <= s <= 0.6]),
                        'negative': len([s for s in scores if s < 0.2])
                    },
                    'platform_sentiments': self._calculate_platform_sentiments(sentiment_data),
                    'detailed_results': sentiment_data[:20]  # Primeiros 20 para economia de espa√ßo
                }
            else:
                sentiment_analysis = {
                    'total_analyzed': 0,
                    'error': 'Nenhum dado v√°lido para an√°lise de sentimento'
                }

            return sentiment_analysis

        except Exception as e:
            logger.error(f"Erro na an√°lise de sentimento: {e}")
            return {'error': str(e)}

    def _perform_trend_analysis(self, extracted_data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Realiza an√°lise de tend√™ncias"""

        try:
            # Identifica hashtags e palavras-chave trending
            hashtags = []
            keywords = []

            for platform_data in extracted_data.get('consolidated_data', {}).values():
                if isinstance(platform_data, list):
                    for item in platform_data:
                        text_content = []

                        if 'title' in item:
                            text_content.append(item['title'])
                        if 'text' in item:
                            text_content.append(item['text'])
                        if 'description' in item:
                            text_content.append(item['description'])

                        combined_text = ' '.join(text_content)

                        # Extrai hashtags
                        found_hashtags = re.findall(r'#\w+', combined_text)
                        hashtags.extend(found_hashtags)

                        # Extrai palavras-chave (palavras de 4+ caracteres)
                        words = re.findall(r'\b\w{4,}\b', combined_text.lower())
                        keywords.extend(words)

            # Conta frequ√™ncias
            hashtag_frequency = Counter(hashtags).most_common(20)
            keyword_frequency = Counter(keywords).most_common(30)

            # An√°lise de tend√™ncias usando IA
            trends_prompt = f"""
            Com base nos seguintes dados extra√≠dos de redes sociais, identifique tend√™ncias emergentes:

            HASHTAGS MAIS FREQUENTES:
            {', '.join([f"{tag}({count})" for tag, count in hashtag_frequency[:15]])}

            PALAVRAS-CHAVE FREQUENTES:
            {', '.join([f"{word}({count})" for word, count in keyword_frequency[:20]])}

            Identifique:
            1. Tend√™ncias emergentes no mercado
            2. Temas em alta
            3. Oportunidades de timing
            4. Padr√µes sazonais identificados
            5. Influenciadores ou marcas em destaque

            Responda de forma estruturada e detalhada.
            """

            trends_insights = ai_manager.generate_content(
                trends_prompt,
                max_tokens=1500,
                temperature=0.4
            )

            trend_analysis = {
                'hashtag_trends': hashtag_frequency,
                'keyword_trends': keyword_frequency,
                'trend_insights': trends_insights,
                'trend_metrics': {
                    'unique_hashtags': len(set(hashtags)),
                    'unique_keywords': len(set(keywords)),
                    'trend_diversity_score': min(100, len(set(hashtags)) + len(set(keywords)))
                }
            }

            return trend_analysis

        except Exception as e:
            logger.error(f"Erro na an√°lise de tend√™ncias: {e}")
            return {'error': str(e)}

    def _perform_competitor_analysis(self, extracted_data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Realiza an√°lise competitiva"""

        try:
            competitors_found = []
            competitor_content = []

            # Identifica poss√≠veis concorrentes nos dados
            for platform_data in extracted_data.get('consolidated_data', {}).values():
                if isinstance(platform_data, list):
                    for item in platform_data:
                        # Busca por indicadores de concorrentes (marcas, empresas)
                        text_to_analyze = ' '.join([
                            item.get('title', ''),
                            item.get('text', ''),
                            item.get('description', '')
                        ])

                        # Identifica poss√≠veis nomes de marcas/empresas
                        potential_brands = re.findall(r'\b[A-Z][a-zA-Z]{2,}\b', text_to_analyze)

                        for brand in potential_brands:
                            if len(brand) > 3 and brand not in ['Para', 'Como', 'Mais', 'Hoje', 'Brasil']:
                                competitors_found.append(brand)

                        competitor_content.append({
                            'content': text_to_analyze[:200],
                            'platform': item.get('platform', 'unknown'),
                            'url': item.get('url', '')
                        })

            # An√°lise competitiva usando IA
            competitor_analysis_prompt = f"""
            Analise o seguinte conte√∫do de redes sociais para identificar insights competitivos:

            POSS√çVEIS CONCORRENTES MENCIONADOS:
            {', '.join(list(set(competitors_found))[:20])}

            AMOSTRA DE CONTE√öDO:
            {chr(10).join([content['content'] for content in competitor_content[:10]])}

            Forne√ßa insights sobre:
            1. Principais concorrentes identificados
            2. Estrat√©gias de conte√∫do dos concorrentes
            3. Pontos fortes e fracos identificados
            4. Oportunidades de diferencia√ß√£o
            5. Gaps no mercado

            Seja espec√≠fico e estrat√©gico.
            """

            competitor_insights = ai_manager.generate_content(
                competitor_analysis_prompt,
                max_tokens=2000,
                temperature=0.3
            )

            competitor_analysis = {
                'potential_competitors': list(set(competitors_found))[:30],
                'competitor_frequency': Counter(competitors_found).most_common(15),
                'competitor_insights': competitor_insights,
                'analysis_metrics': {
                    'total_competitors_mentioned': len(competitors_found),
                    'unique_competitors': len(set(competitors_found)),
                    'content_pieces_analyzed': len(competitor_content)
                }
            }

            return competitor_analysis

        except Exception as e:
            logger.error(f"Erro na an√°lise competitiva: {e}")
            return {'error': str(e)}

    def _perform_opportunity_analysis(self, extracted_data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Realiza an√°lise de oportunidades"""

        try:
            # Coleta dados para an√°lise de oportunidades
            all_content = []
            engagement_indicators = []

            for platform_data in extracted_data.get('consolidated_data', {}).values():
                if isinstance(platform_data, list):
                    for item in platform_data:
                        content_piece = {
                            'text': ' '.join([
                                item.get('title', ''),
                                item.get('text', ''),
                                item.get('description', '')
                            ]),
                            'platform': item.get('platform', ''),
                            'url': item.get('url', '')
                        }
                        all_content.append(content_piece)

            # An√°lise de oportunidades usando IA
            opportunity_prompt = f"""
            Com base no seguinte conte√∫do de redes sociais, identifique oportunidades de neg√≥cio:

            AMOSTRA DE CONTE√öDO ANALISADO:
            {chr(10).join([content['text'][:150] for content in all_content[:15]])}

            PLATAFORMAS ANALISADAS:
            {', '.join(set([content['platform'] for content in all_content if content['platform']]))}

            Identifique e detalhe:
            1. Oportunidades de mercado n√£o exploradas
            2. Necessidades do p√∫blico n√£o atendidas
            3. Gaps de conte√∫do identificados
            4. Oportunidades de parcerias
            5. Nichos promissores
            6. Tend√™ncias emergentes para aproveitar
            7. Timing ideal para entrada no mercado

            Para cada oportunidade, forne√ßa:
            - Descri√ß√£o da oportunidade
            - Potencial de mercado
            - Estrat√©gia sugerida
            - Pr√≥ximos passos recomendados
            """

            opportunity_insights = ai_manager.generate_content(
                opportunity_prompt,
                max_tokens=2500,
                temperature=0.4
            )

            opportunity_analysis = {
                'total_content_analyzed': len(all_content),
                'platforms_coverage': list(set([content['platform'] for content in all_content if content['platform']])),
                'opportunity_insights': opportunity_insights,
                'opportunity_metrics': {
                    'content_diversity_score': len(set([content['platform'] for content in all_content])) * 10,
                    'market_coverage_score': min(100, len(all_content) * 2),
                    'opportunity_potential_score': min(100, len(all_content) * 3)
                }
            }

            return opportunity_analysis

        except Exception as e:
            logger.error(f"Erro na an√°lise de oportunidades: {e}")
            return {'error': str(e)}

    def _perform_audience_analysis(self, extracted_data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Realiza an√°lise de audi√™ncia"""

        try:
            audience_indicators = []
            content_preferences = []

            for platform, platform_data in extracted_data.get('consolidated_data', {}).items():
                if isinstance(platform_data, list):
                    for item in platform_data:
                        content_text = ' '.join([
                            item.get('title', ''),
                            item.get('text', ''),
                            item.get('description', '')
                        ])

                        # Identifica indicadores de audi√™ncia
                        age_indicators = re.findall(r'\b(?:jovem|adulto|idoso|teen|millennial|gera√ß√£o|idade)\b', content_text.lower())
                        interest_indicators = re.findall(r'\b(?:gosta|adora|prefere|interesse|hobby|paix√£o)\w*\b', content_text.lower())

                        audience_indicators.extend(age_indicators + interest_indicators)

                        content_preferences.append({
                            'platform': platform,
                            'content_type': self._identify_content_type(item),
                            'engagement_style': self._identify_engagement_style(content_text)
                        })

            # An√°lise de audi√™ncia usando IA
            audience_prompt = f"""
            Analise o seguinte conte√∫do de redes sociais para criar um perfil detalhado da audi√™ncia:

            INDICADORES DE AUDI√äNCIA ENCONTRADOS:
            {', '.join(audience_indicators[:30])}

            PREFER√äNCIAS DE CONTE√öDO POR PLATAFORMA:
            {chr(10).join([f"{pref['platform']}: {pref['content_type']}" for pref in content_preferences[:20]])}

            Crie um perfil detalhado da audi√™ncia incluindo:
            1. Demografia prov√°vel (idade, g√™nero, localiza√ß√£o)
            2. Interesses e comportamentos
            3. Prefer√™ncias de conte√∫do
            4. Padr√µes de engajamento
            5. Linguagem e tom preferidos
            6. Hor√°rios de maior atividade
            7. Motiva√ß√µes e necessidades
            8. Personas espec√≠ficas identificadas

            Seja espec√≠fico e baseie-se nos dados analisados.
            """

            audience_insights = ai_manager.generate_content(
                audience_prompt,
                max_tokens=2000,
                temperature=0.3
            )

            audience_analysis = {
                'audience_indicators': Counter(audience_indicators).most_common(20),
                'platform_preferences': Counter([pref['platform'] for pref in content_preferences]).most_common(),
                'content_type_preferences': Counter([pref['content_type'] for pref in content_preferences]).most_common(),
                'audience_insights': audience_insights,
                'audience_metrics': {
                    'total_indicators_found': len(audience_indicators),
                    'platform_diversity': len(set([pref['platform'] for pref in content_preferences])),
                    'audience_profile_confidence': min(100, len(audience_indicators) * 2)
                }
            }

            return audience_analysis

        except Exception as e:
            logger.error(f"Erro na an√°lise de audi√™ncia: {e}")
            return {'error': str(e)}

    def _perform_market_analysis(self, extracted_data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Realiza an√°lise de mercado"""

        try:
            market_indicators = []
            price_mentions = []
            location_mentions = []

            for platform_data in extracted_data.get('consolidated_data', {}).values():
                if isinstance(platform_data, list):
                    for item in platform_data:
                        content_text = ' '.join([
                            item.get('title', ''),
                            item.get('text', ''),
                            item.get('description', '')
                        ]).lower()

                        # Identifica men√ß√µes de pre√ßo
                        prices = re.findall(r'r\$\s*\d+(?:,\d+)?|\d+\s*reais?', content_text)
                        price_mentions.extend(prices)

                        # Identifica localiza√ß√µes
                        locations = re.findall(r'\b(?:s√£o paulo|rio de janeiro|brasil|belo horizonte|salvador|bras√≠lia|fortaleza|recife|curitiba|porto alegre)\b', content_text)
                        location_mentions.extend(locations)

                        # Identifica indicadores de mercado
                        market_terms = re.findall(r'\b(?:mercado|vendas|lucro|crescimento|economia|neg√≥cio|empresa|startup|investimento)\b', content_text)
                        market_indicators.extend(market_terms)

            # An√°lise de mercado usando IA
            market_prompt = f"""
            Analise os seguintes dados de mercado extra√≠dos de redes sociais:

            INDICADORES DE MERCADO:
            {', '.join(list(set(market_indicators))[:25])}

            MEN√á√ïES DE PRE√áOS:
            {', '.join(price_mentions[:15])}

            LOCALIZA√á√ïES MENCIONADAS:
            {', '.join(list(set(location_mentions))[:15])}

            Forne√ßa uma an√°lise abrangente do mercado incluindo:
            1. Tamanho e potencial do mercado
            2. Tend√™ncias de pre√ßos identificadas
            3. Distribui√ß√£o geogr√°fica
            4. Principais players do mercado
            5. Barreiras de entrada
            6. Fatores de crescimento
            7. Riscos e desafios
            8. Oportunidades regionais espec√≠ficas

            Base sua an√°lise nos dados coletados.
            """

            market_insights = ai_manager.generate_content(
                market_prompt,
                max_tokens=2000,
                temperature=0.3
            )

            market_analysis = {
                'market_indicators': Counter(market_indicators).most_common(20),
                'price_mentions': price_mentions[:20],
                'geographic_distribution': Counter(location_mentions).most_common(),
                'market_insights': market_insights,
                'market_metrics': {
                    'market_mentions_total': len(market_indicators),
                    'price_data_points': len(price_mentions),
                    'geographic_coverage': len(set(location_mentions)),
                    'market_maturity_score': min(100, len(market_indicators) * 1.5)
                }
            }

            return market_analysis

        except Exception as e:
            logger.error(f"Erro na an√°lise de mercado: {e}")
            return {'error': str(e)}

    def _perform_behavioral_analysis(self, extracted_data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Realiza an√°lise comportamental"""

        try:
            behavior_patterns = []
            engagement_patterns = []

            for platform, platform_data in extracted_data.get('consolidated_data', {}).items():
                if isinstance(platform_data, list):
                    for item in platform_data:
                        content_text = ' '.join([
                            item.get('title', ''),
                            item.get('text', ''),
                            item.get('description', '')
                        ]).lower()

                        # Identifica padr√µes comportamentais
                        behaviors = re.findall(r'\b(?:compra|comprou|usa|utiliza|prefere|escolhe|decide|busca|procura|quer|precisa|deseja)\w*\b', content_text)
                        behavior_patterns.extend(behaviors)

                        # Analisa padr√µes de engajamento
                        engagement_words = re.findall(r'\b(?:curtiu|comentou|compartilhou|seguiu|marcou|repostou|salvou)\w*\b', content_text)
                        engagement_patterns.extend(engagement_words)

            # An√°lise comportamental usando IA
            behavioral_prompt = f"""
            Analise os seguintes padr√µes comportamentais extra√≠dos de redes sociais:

            PADR√ïES COMPORTAMENTAIS:
            {', '.join(list(set(behavior_patterns))[:25])}

            PADR√ïES DE ENGAJAMENTO:
            {', '.join(list(set(engagement_patterns))[:20])}

            Forne√ßa uma an√°lise detalhada dos comportamentos incluindo:
            1. Jornada do consumidor identificada
            2. Pontos de decis√£o cr√≠ticos
            3. Motivadores de compra
            4. Barreiras comportamentais
            5. Padr√µes de engajamento por plataforma
            6. Gatilhos emocionais identificados
            7. Influenciadores de decis√£o
            8. Momentos ideais para abordagem

            Base sua an√°lise nos dados comportamentais coletados.
            """

            behavioral_insights = ai_manager.generate_content(
                behavioral_prompt,
                max_tokens=2000,
                temperature=0.3
            )

            behavioral_analysis = {
                'behavior_patterns': Counter(behavior_patterns).most_common(20),
                'engagement_patterns': Counter(engagement_patterns).most_common(15),
                'behavioral_insights': behavioral_insights,
                'behavioral_metrics': {
                    'total_behaviors_identified': len(behavior_patterns),
                    'engagement_diversity': len(set(engagement_patterns)),
                    'behavioral_confidence_score': min(100, len(behavior_patterns) * 1.8)
                }
            }

            return behavioral_analysis

        except Exception as e:
            logger.error(f"Erro na an√°lise comportamental: {e}")
            return {'error': str(e)}

    def _consolidate_insights(self, detailed_analysis: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Consolida todos os insights em uma vis√£o unificada"""

        try:
            # Extrai insights-chave de cada an√°lise
            key_insights = []

            for analysis_type, analysis_data in detailed_analysis.items():
                if isinstance(analysis_data, dict) and 'error' not in analysis_data:
                    # Extrai insights espec√≠ficos de cada tipo de an√°lise
                    if analysis_type == 'content_analysis':
                        key_insights.append(f"An√°lise de Conte√∫do: {analysis_data.get('total_content_pieces', 0)} pe√ßas analisadas")
                    elif analysis_type == 'sentiment_analysis':
                        avg_sentiment = analysis_data.get('average_sentiment', 0)
                        sentiment_label = self._score_to_label(avg_sentiment)
                        key_insights.append(f"Sentimento Geral: {sentiment_label} (score: {avg_sentiment:.2f})")
                    elif analysis_type == 'trend_analysis':
                        top_trend = analysis_data.get('hashtag_trends', [])
                        if top_trend:
                            key_insights.append(f"Tend√™ncia Principal: {top_trend[0][0]} ({top_trend[0][1]} men√ß√µes)")
                    elif analysis_type == 'opportunity_analysis':
                        score = analysis_data.get('opportunity_metrics', {}).get('opportunity_potential_score', 0)
                        key_insights.append(f"Potencial de Oportunidades: {score}/100")

            # Gera consolida√ß√£o final usando IA
            consolidation_prompt = f"""
            Com base nas seguintes an√°lises detalhadas, forne√ßa insights consolidados e recomenda√ß√µes estrat√©gicas:

            AN√ÅLISES REALIZADAS:
            {chr(10).join([f"- {analysis_type}: Conclu√≠da" for analysis_type in detailed_analysis.keys()])}

            INSIGHTS-CHAVE IDENTIFICADOS:
            {chr(10).join(key_insights)}

            Forne√ßa uma consolida√ß√£o estrat√©gica incluindo:
            1. Top 5 insights mais importantes
            2. Recomenda√ß√µes estrat√©gicas priorit√°rias
            3. Oportunidades de a√ß√£o imediata
            4. Riscos a serem monitorados
            5. KPIs recomendados para acompanhamento
            6. Pr√≥ximos passos sugeridos

            Seja espec√≠fico, estrat√©gico e acion√°vel.
            """

            consolidated_insights_ai = ai_manager.generate_content(
                consolidation_prompt,
                max_tokens=2000,
                temperature=0.4
            )

            consolidated_insights = {
                'key_insights': key_insights,
                'ai_consolidated_insights': consolidated_insights_ai,
                'analysis_summary': {
                    'total_analyses_completed': len(detailed_analysis),
                    'analyses_with_errors': len([a for a in detailed_analysis.values() if isinstance(a, dict) and 'error' in a]),
                    'overall_success_rate': (len(detailed_analysis) - len([a for a in detailed_analysis.values() if isinstance(a, dict) and 'error' in a])) / max(len(detailed_analysis), 1) * 100
                },
                'consolidation_timestamp': datetime.now().isoformat()
            }

            return consolidated_insights

        except Exception as e:
            logger.error(f"Erro na consolida√ß√£o de insights: {e}")
            return {'error': str(e)}

    def _analyze_word_frequency(self, text: str) -> List[tuple]:
        """Analisa frequ√™ncia de palavras no texto"""

        try:
            # Remove pontua√ß√£o e converte para min√∫sculas
            clean_text = re.sub(r'[^\w\s]', ' ', text.lower())
            words = clean_text.split()

            # Remove palavras muito comuns (stop words)
            stop_words = {'a', 'o', 'e', 'de', 'da', 'do', 'em', 'um', 'uma', 'com', 'para', 'por', 'que', 'se', 'na', 'no', 'as', 'os', 'mais', 'como', 'ser', 'ter', 'foi', 's√£o', 'est√°', 'muito', 'bem', 'quando', 'onde', 'qual', 'quem', 'porque', 'isso', 'isso', 'essa', 'esse'}

            filtered_words = [word for word in words if word not in stop_words and len(word) > 3]

            return Counter(filtered_words).most_common(50)

        except Exception as e:
            logger.error(f"Erro na an√°lise de frequ√™ncia: {e}")
            return []

    def _extract_sentiment_score(self, ai_response: str) -> float:
        """Extrai score num√©rico da resposta de IA"""

        try:
            # Busca por n√∫meros entre 0 e 1
            numbers = re.findall(r'0\.\d+|1\.0|0\.0', ai_response)
            if numbers:
                return float(numbers[0])

            # Busca por n√∫meros inteiros que podem ser convertidos
            if 'positivo' in ai_response.lower():
                return 0.8
            elif 'negativo' in ai_response.lower():
                return 0.2
            else:
                return 0.5

        except Exception:
            return 0.5

    def _score_to_label(self, score: float) -> str:
        """Converte score num√©rico em label de sentimento"""

        if score > 0.6:
            return 'Positivo'
        elif score < 0.2:
            return 'Negativo'
        else:
            return 'Neutro'

    def _calculate_platform_sentiments(self, sentiment_data: List[Dict]) -> Dict[str, float]:
        """Calcula sentimento m√©dio por plataforma"""

        platform_scores = {}

        for item in sentiment_data:
            platform = item.get('platform', 'unknown')
            score = item.get('sentiment_score', 0.5)

            if platform not in platform_scores:
                platform_scores[platform] = []

            platform_scores[platform].append(score)

        # Calcula m√©dias
        platform_averages = {}
        for platform, scores in platform_scores.items():
            platform_averages[platform] = sum(scores) / len(scores)

        return platform_averages

    def _identify_content_type(self, item: Dict[str, Any]) -> str:
        """Identifica tipo de conte√∫do"""

        url = item.get('url', '').lower()
        title = item.get('title', '').lower()

        if 'youtube.com' in url or 'youtu.be' in url:
            return 'video'
        elif 'instagram.com' in url:
            return 'social_post'
        elif 'tiktok.com' in url:
            return 'short_video'
        elif 'twitter.com' in url or 'x.com' in url:
            return 'tweet'
        elif any(word in title for word in ['tutorial', 'como fazer', 'dica']):
            return 'educational'
        elif any(word in title for word in ['review', 'an√°lise', 'teste']):
            return 'review'
        else:
            return 'general'

    def _identify_engagement_style(self, text: str) -> str:
        """Identifica estilo de engajamento"""

        text_lower = text.lower()

        if any(word in text_lower for word in ['pergunta', '?', 'o que acham']):
            return 'interactive'
        elif any(word in text_lower for word in ['tutorial', 'passo a passo', 'como']):
            return 'educational'
        elif any(word in text_lower for word in ['oferta', 'desconto', 'promo√ß√£o']):
            return 'promotional'
        elif any(word in text_lower for word in ['hist√≥ria', 'experi√™ncia', 'aconteceu']):
            return 'storytelling'
        else:
            return 'informational'

    def _calculate_data_quality_score(self, analysis_results: Dict[str, Any]) -> int:
        """Calcula score de qualidade dos dados"""

        try:
            score = 0

            # Pontua√ß√£o baseada no n√∫mero de an√°lises completadas
            completed_analyses = len([a for a in analysis_results.get('detailed_analysis', {}).values() if isinstance(a, dict) and 'error' not in a])
            score += completed_analyses * 10

            # Pontua√ß√£o baseada no volume de dados
            input_summary = analysis_results.get('input_data_summary', {})
            data_volume = input_summary.get('data_volume_score', 0)
            score += min(30, data_volume)

            # Pontua√ß√£o baseada na diversidade de plataformas
            platforms = len(input_summary.get('platforms_analyzed', []))
            score += platforms * 5

            return min(100, score)

        except Exception:
            return 50

    def analyze_comprehensive_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """An√°lise ultra-abrangente dos dados coletados - TEMPO EXPANDIDO PARA QUALIDADE"""

        try:
            logger.info("üî¨ Iniciando an√°lise ULTRA-PROFUNDA dos dados (tempo estendido para m√°xima qualidade)...")

            analysis_start = time.time()

            # SEMPRE faz an√°lise completa e profunda - sem atalhos
            comprehensive_analysis = {}

            # Fase 1: An√°lise estrutural dos dados
            comprehensive_analysis.update(self._perform_structural_analysis(data))

            # Fase 2: An√°lise sem√¢ntica profunda
            comprehensive_analysis.update(self._perform_semantic_deep_analysis(data))

            # Fase 3: An√°lise contextual avan√ßada
            comprehensive_analysis.update(self._perform_contextual_analysis(data))

            # Fase 4: An√°lise preditiva
            comprehensive_analysis.update(self._perform_predictive_analysis(data))

            # Fase 5: S√≠ntese final inteligente
            comprehensive_analysis.update(self._perform_intelligent_synthesis(data))

            analysis_time = time.time() - analysis_start
            logger.info(f"‚úÖ An√°lise ULTRA-PROFUNDA conclu√≠da em {analysis_time:.2f}s (qualidade > velocidade)")

            return comprehensive_analysis

        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise abrangente: {str(e)}")
            return self._create_fallback_analysis()

    def _perform_structural_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """An√°lise estrutural dos dados"""
        return {
            'structural_analysis': {
                'data_completeness': self._assess_data_completeness(data),
                'data_quality_score': self._calculate_data_quality(data),
                'information_density': self._calculate_information_density(data)
            }
        }

    def _perform_semantic_deep_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """An√°lise sem√¢ntica profunda"""
        return {
            'semantic_analysis': {
                'key_themes': self._extract_key_themes(data),
                'sentiment_patterns': self._analyze_sentiment_patterns(data),
                'language_complexity': self._assess_language_complexity(data)
            }
        }

    def _perform_contextual_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """An√°lise contextual avan√ßada"""
        return {
            'contextual_analysis': {
                'market_context': self._analyze_market_context(data),
                'competitive_landscape': self._analyze_competitive_context(data),
                'temporal_patterns': self._analyze_temporal_patterns(data)
            }
        }

    def _perform_predictive_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """An√°lise preditiva"""
        return {
            'predictive_analysis': {
                'trend_predictions': self._predict_trends(data),
                'opportunity_identification': self._identify_opportunities(data),
                'risk_assessment': self._assess_risks(data)
            }
        }

    def _perform_intelligent_synthesis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """S√≠ntese final inteligente"""
        return {
            'intelligent_synthesis': {
                'key_insights': self._synthesize_key_insights(data),
                'actionable_recommendations': self._generate_actionable_recommendations(data),
                'strategic_implications': self._analyze_strategic_implications(data)
            }
        }

    # M√©todos auxiliares placeholders (devem ser implementados com l√≥gica real)
    def _assess_data_completeness(self, data: Dict[str, Any]) -> str:
        """Avalia a completude dos dados"""
        return "Completo"

    def _calculate_data_quality(self, data: Dict[str, Any]) -> int:
        """Calcula a qualidade dos dados"""
        return 95

    def _calculate_information_density(self, data: Dict[str, Any]) -> float:
        """Calcula a densidade de informa√ß√£o"""
        return 0.85

    def _extract_key_themes(self, data: Dict[str, Any]) -> List[str]:
        """Extrai temas chave dos dados"""
        return ["Inova√ß√£o", "Sustentabilidade", "Marketing Digital"]

    def _analyze_sentiment_patterns(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa padr√µes de sentimento"""
        return {"geral": "Positivo", "tendencia": "Levemente Positivo"}

    def _assess_language_complexity(self, data: Dict[str, Any]) -> str:
        """Avalia a complexidade da linguagem"""
        return "M√©dia"

    def _analyze_market_context(self, data: Dict[str, Any]) -> str:
        """Analisa o contexto de mercado"""
        return "Mercado em crescimento com alta competitividade"

    def _analyze_competitive_context(self, data: Dict[str, Any]) -> str:
        """Analisa o contexto competitivo"""
        return "Concorr√™ncia intensa em nichos espec√≠ficos"

    def _analyze_temporal_patterns(self, data: Dict[str, Any]) -> str:
        """Analisa padr√µes temporais"""
        return "Picos de atividade em dias √∫teis e hor√°rios comerciais"

    def _predict_trends(self, data: Dict[str, Any]) -> List[str]:
        """Prev√™ tend√™ncias futuras"""
        return ["IA Generativa", "Economia Circular", "Trabalho H√≠brido"]

    def _identify_opportunities(self, data: Dict[str, Any]) -> List[str]:
        """Identifica oportunidades"""
        return ["Expans√£o para mercados emergentes", "Desenvolvimento de produtos eco-friendly"]

    def _assess_risks(self, data: Dict[str, Any]) -> List[str]:
        """Avalia riscos"""
        return ["Instabilidade regulat√≥ria", "Mudan√ßas no comportamento do consumidor"]

    def _synthesize_key_insights(self, data: Dict[str, Any]) -> List[str]:
        """Sintetiza insights chave"""
        return ["Oportunidade de lideran√ßa em IA", "Necessidade de otimiza√ß√£o de custos"]

    def _generate_actionable_recommendations(self, data: Dict[str, Any]) -> List[str]:
        """Gera recomenda√ß√µes acion√°veis"""
        return ["Investir em P&D de IA", "Lan√ßar campanha de marketing focada em sustentabilidade"]

    def _analyze_strategic_implications(self, data: Dict[str, Any]) -> str:
        """Analisa implica√ß√µes estrat√©gicas"""
        return "Posicionamento estrat√©gico para lideran√ßa de mercado a longo prazo"

    def _create_fallback_analysis(self) -> Dict[str, Any]:
        """Cria uma an√°lise de fallback em caso de erro grave"""
        return {
            "error": "Falha cr√≠tica na an√°lise abrangente. N√£o foi poss√≠vel gerar resultados.",
            "fallback_analysis": True,
            "timestamp": datetime.now().isoformat()
        }

# Inst√¢ncia global
enhanced_data_analyzer = EnhancedDataAnalyzer()